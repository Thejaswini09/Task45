{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('contract', 'NN')\n",
      "(PERSON Birla/NNP Corporation/NNP Hindustan/NNP)\n",
      "('Inc.', 'NNP')\n",
      "('signed', 'VBD')\n",
      "('March', 'NNP')\n",
      "('10', 'CD')\n",
      "(',', ',')\n",
      "('2020', 'CD')\n",
      "('.', '.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"The contract between Birla Corporation and Hindustan Inc. was signed on March 10, 2020.\"\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_filtered = [w for w in tokens if not w.lower() in stop_words]\n",
    "length = len(tokens_filtered)\n",
    "for i in range (1, length):\n",
    "\tif i in (',', '.', '_'):\n",
    "\t\ttokens_filtered.remove(tokens_filtered[i])\n",
    "    \n",
    "\n",
    "# Apply part-of-speech tagging to the tokens\n",
    "tagged = nltk.pos_tag(tokens_filtered)\n",
    "\n",
    "# Apply named entity recognition to the tagged words\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "# Print the entities found in the text\n",
    "for entity in entities:\n",
    "\tprint(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Birla Corporation Hindustan, Type: PERSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"The contract between Birla Corporation and Hindustan Inc. was signed on March 10, 2020.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_filtered = [w for w in tokens if not w.lower() in stop_words and w not in punctuation]\n",
    "\n",
    "# Apply part-of-speech tagging to the filtered tokens\n",
    "tagged = nltk.pos_tag(tokens_filtered)\n",
    "\n",
    "# Apply named entity recognition to the tagged words\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "# Print the named entities found in the text\n",
    "for entity in entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(f\"Entity: {' '.join(c[0] for c in entity.leaves())}, Type: {entity.label()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.87120        0.017\n",
      "             2          -1.68949        1.000\n",
      "             3          -1.03845        1.000\n",
      "             4          -0.71560        1.000\n",
      "             5          -0.53704        1.000\n",
      "             6          -0.42682        1.000\n",
      "             7          -0.35291        1.000\n",
      "             8          -0.30023        1.000\n",
      "             9          -0.26093        1.000\n",
      "            10          -0.23055        1.000\n",
      "            11          -0.20640        1.000\n",
      "            12          -0.18677        1.000\n",
      "            13          -0.17050        1.000\n",
      "            14          -0.15681        1.000\n",
      "            15          -0.14514        1.000\n",
      "            16          -0.13507        1.000\n",
      "            17          -0.12629        1.000\n",
      "            18          -0.11858        1.000\n",
      "            19          -0.11175        1.000\n",
      "            20          -0.10566        1.000\n",
      "            21          -0.10020        1.000\n",
      "            22          -0.09527        1.000\n",
      "            23          -0.09080        1.000\n",
      "            24          -0.08674        1.000\n",
      "            25          -0.08301        1.000\n",
      "            26          -0.07960        1.000\n",
      "            27          -0.07645        1.000\n",
      "            28          -0.07355        1.000\n",
      "            29          -0.07085        1.000\n",
      "            30          -0.06835        1.000\n",
      "            31          -0.06601        1.000\n",
      "            32          -0.06383        1.000\n",
      "            33          -0.06179        1.000\n",
      "            34          -0.05988        1.000\n",
      "            35          -0.05808        1.000\n",
      "            36          -0.05639        1.000\n",
      "            37          -0.05479        1.000\n",
      "            38          -0.05328        1.000\n",
      "            39          -0.05185        1.000\n",
      "            40          -0.05049        1.000\n",
      "            41          -0.04921        1.000\n",
      "            42          -0.04799        1.000\n",
      "            43          -0.04683        1.000\n",
      "            44          -0.04572        1.000\n",
      "            45          -0.04466        1.000\n",
      "            46          -0.04366        1.000\n",
      "            47          -0.04269        1.000\n",
      "            48          -0.04177        1.000\n",
      "            49          -0.04089        1.000\n",
      "            50          -0.04004        1.000\n",
      "            51          -0.03923        1.000\n",
      "            52          -0.03845        1.000\n",
      "            53          -0.03770        1.000\n",
      "            54          -0.03698        1.000\n",
      "            55          -0.03629        1.000\n",
      "            56          -0.03562        1.000\n",
      "            57          -0.03498        1.000\n",
      "            58          -0.03436        1.000\n",
      "            59          -0.03376        1.000\n",
      "            60          -0.03318        1.000\n",
      "            61          -0.03262        1.000\n",
      "            62          -0.03208        1.000\n",
      "            63          -0.03156        1.000\n",
      "            64          -0.03105        1.000\n",
      "            65          -0.03056        1.000\n",
      "            66          -0.03009        1.000\n",
      "            67          -0.02963        1.000\n",
      "            68          -0.02918        1.000\n",
      "            69          -0.02875        1.000\n",
      "            70          -0.02833        1.000\n",
      "            71          -0.02792        1.000\n",
      "            72          -0.02752        1.000\n",
      "            73          -0.02714        1.000\n",
      "            74          -0.02676        1.000\n",
      "            75          -0.02640        1.000\n",
      "            76          -0.02605        1.000\n",
      "            77          -0.02570        1.000\n",
      "            78          -0.02537        1.000\n",
      "            79          -0.02504        1.000\n",
      "            80          -0.02472        1.000\n",
      "            81          -0.02441        1.000\n",
      "            82          -0.02410        1.000\n",
      "            83          -0.02381        1.000\n",
      "            84          -0.02352        1.000\n",
      "            85          -0.02324        1.000\n",
      "            86          -0.02296        1.000\n",
      "            87          -0.02270        1.000\n",
      "            88          -0.02243        1.000\n",
      "            89          -0.02218        1.000\n",
      "            90          -0.02193        1.000\n",
      "            91          -0.02168        1.000\n",
      "            92          -0.02144        1.000\n",
      "            93          -0.02121        1.000\n",
      "            94          -0.02098        1.000\n",
      "            95          -0.02076        1.000\n",
      "            96          -0.02054        1.000\n",
      "            97          -0.02032        1.000\n",
      "            98          -0.02011        1.000\n",
      "            99          -0.01991        1.000\n",
      "         Final          -0.01970        1.000\n",
      "['The', 'Birla', 'Corporation', 'in', 'in', 'ruled']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example training data (replace with your domain-specific data)\n",
    "training_data = [\n",
    "   ( \"The contract between Birla Corporation and Hindustan Inc. was signed on March 10, 2020.\",{\"entities\": [( 21, 38,\"CORPORATION\"), (43, 57, \"CORPORATION\"), (72, 86, \"DATE\")]}),\n",
    "  (\"The court in New York ruled in favor of Plaintiff in the case Doe v. Smith.\",{\"entities\": [(13,21, \"LOCATION\"),(40, 49,\"PERSON\"),(62, 74, \"PERSON\")]}),\n",
    "  (\"The shareholders agreed to invest $5 million in Willhome Ventures.\",{\"entities\": [(34, 44, \"CURRENCY\"), (48, 65,  \"CORPORATION\")]}),\n",
    "  (\"The contract stated that payment must be made within 30 days of signing.\",{\"entities\": [(53, 60, \"TIMEFRAME\")]})\n",
    "]\n",
    "\n",
    "# Function to extract features from text\n",
    "def features(tokens, index):\n",
    "    return {\n",
    "        'word': tokens[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(tokens) - 1,\n",
    "        'is_capitalized': tokens[index][0].upper() == tokens[index][0],\n",
    "        'is_all_caps': tokens[index].upper() == tokens[index],\n",
    "        'is_all_lower': tokens[index].lower() == tokens[index],\n",
    "        'prefix-1': tokens[index][0],\n",
    "        'prefix-2': tokens[index][:2],\n",
    "        'prefix-3': tokens[index][:3],\n",
    "        'suffix-1': tokens[index][-1],\n",
    "        'suffix-2': tokens[index][-2:],\n",
    "        'suffix-3': tokens[index][-3:],\n",
    "        'prev_word': '' if index == 0 else tokens[index - 1],\n",
    "        'next_word': '' if index == len(tokens) - 1 else tokens[index + 1],\n",
    "        'has_hyphen': '-' in tokens[index],\n",
    "        'is_numeric': tokens[index].isdigit(),\n",
    "        'capitals_inside': tokens[index][1:].lower() != tokens[index][1:]\n",
    "    }\n",
    "\n",
    "# Function to transform data into features and labels\n",
    "def transform_to_dataset(training_data):\n",
    "    X, y = [], []\n",
    "    for sentence, entities in training_data:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        entities_list = entities.get('entities', [])\n",
    "        for index in range(len(tokens)):\n",
    "            token_features = features(tokens, index)\n",
    "            X.append((token_features, tokens[index]))\n",
    "            entity_label = 'O'  # Default label if no entity\n",
    "            for start, end, label in entities_list:\n",
    "                if index >= start and index < end:\n",
    "                    entity_label = label\n",
    "                    break\n",
    "            y.append(entity_label)\n",
    "    return X, y\n",
    "\n",
    "# Convert training data to NLTK's format\n",
    "X, y = transform_to_dataset(training_data)\n",
    "\n",
    "# Train a classifier (e.g., MaxEnt classifier)\n",
    "classifier = nltk.MaxentClassifier.train(X)\n",
    "\n",
    "# Example of predicting entities\n",
    "sentence = \"The Birla Corporation is legally entitled\"\n",
    "tokens = word_tokenize(sentence)\n",
    "predicted_entities = [classifier.classify(features(tokens, index)) for index in range(len(tokens))]\n",
    "print(predicted_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -2.56495        0.059\n",
      "             2          -1.26425        1.000\n",
      "             3          -0.81614        1.000\n",
      "             4          -0.58791        1.000\n",
      "             5          -0.45516        1.000\n",
      "             6          -0.36970        1.000\n",
      "             7          -0.31053        1.000\n",
      "             8          -0.26732        1.000\n",
      "             9          -0.23446        1.000\n",
      "         Final          -0.20868        1.000\n",
      "['John', 'works', 'at', 'Apple', 'Inc.', 'in', 'California', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example training data (replace with your domain-specific data)\n",
    "training_data = [\n",
    "    (\"John works at Apple Inc. in California.\", {\"entities\": [(0, 4, \"PERSON\"), (14, 23, \"ORG\"), (27, 37, \"GPE\")]}),\n",
    "    (\"Jane is studying at Stanford University in California.\", {\"entities\": [(0, 4, \"PERSON\"), (20, 38, \"ORG\"), (42, 52, \"GPE\")]}),\n",
    "    # Add more training examples as needed\n",
    "]\n",
    "\n",
    "# Function to extract features from text\n",
    "def features(tokens, index):\n",
    "    return {\n",
    "        'word': tokens[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(tokens) - 1,\n",
    "        'is_capitalized': tokens[index][0].upper() == tokens[index][0],\n",
    "        'is_all_caps': tokens[index].upper() == tokens[index],\n",
    "        'is_all_lower': tokens[index].lower() == tokens[index],\n",
    "        'prefix-1': tokens[index][0],\n",
    "        'prefix-2': tokens[index][:2],\n",
    "        'prefix-3': tokens[index][:3],\n",
    "        'suffix-1': tokens[index][-1],\n",
    "        'suffix-2': tokens[index][-2:],\n",
    "        'suffix-3': tokens[index][-3:],\n",
    "        'prev_word': '' if index == 0 else tokens[index - 1],\n",
    "        'next_word': '' if index == len(tokens) - 1 else tokens[index + 1],\n",
    "        'has_hyphen': '-' in tokens[index],\n",
    "        'is_numeric': tokens[index].isdigit(),\n",
    "        'capitals_inside': tokens[index][1:].lower() != tokens[index][1:]\n",
    "    }\n",
    "\n",
    "# Function to transform data into features and labels\n",
    "def transform_to_dataset(training_data):\n",
    "    X, y = [], []\n",
    "    for sentence, entities in training_data:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        entities_list = entities.get('entities', [])\n",
    "        for index in range(len(tokens)):\n",
    "            token_features = features(tokens, index)\n",
    "            X.append((token_features, tokens[index]))\n",
    "            entity_label = 'O'  # Default label if no entity\n",
    "            for start, end, label in entities_list:\n",
    "                if index >= start and index < end:\n",
    "                    entity_label = label\n",
    "                    break\n",
    "            y.append(entity_label)\n",
    "    return X, y\n",
    "\n",
    "# Convert training data to NLTK's format\n",
    "X, y = transform_to_dataset(training_data)\n",
    "\n",
    "# Train a classifier (e.g., MaxEnt classifier)\n",
    "classifier = nltk.MaxentClassifier.train(X, max_iter=10)\n",
    "\n",
    "# Example of predicting entities\n",
    "sentence = \"John works at Apple Inc. in California.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "predicted_entities = [classifier.classify(features(tokens, index)) for index in range(len(tokens))]\n",
    "print(predicted_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
