{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' Byju’s had been a partner of the Indian cricket team since 2019, with its branding featured on the front of the team’s jersey. In June last year, Byju’s extended its sponsorship rights with the BCCI till November. \n",
    "The ed-tech firm had asked the cricket board to encash a ₹140 crore bank guarantee, with the remaining ₹160 crore to be paid in instalments.\n",
    "In November, Byju’s hinted at possible talks on a settlement between the two. However, the company has been facing a severe financial crisis and is unable to repay debts owed to its creditors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byju’s --> ORG --> Companies, agencies, institutions, etc.\n",
      "Indian --> NORP --> Nationalities or religious or political groups\n",
      "2019 --> DATE --> Absolute or relative dates or periods\n",
      "June last year --> DATE --> Absolute or relative dates or periods\n",
      "Byju --> PERSON --> People, including fictional\n",
      "BCCI --> ORG --> Companies, agencies, institutions, etc.\n",
      "November --> DATE --> Absolute or relative dates or periods\n",
      "₹ --> ORG --> Companies, agencies, institutions, etc.\n",
      "160 --> MONEY --> Monetary values, including unit\n",
      "November --> DATE --> Absolute or relative dates or periods\n",
      "Byju --> PERSON --> People, including fictional\n",
      "two --> CARDINAL --> Numerals that do not fall under another type\n"
     ]
    }
   ],
   "source": [
    "for ents in doc.ents:\n",
    "    print(ents,\"-->\",ents.label_,\"-->\",spacy.explain(ents.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Byju/NNP)\n",
      "('’', 'NNP')\n",
      "('partner', 'NN')\n",
      "(GPE Indian/JJ)\n",
      "('cricket', 'NN')\n",
      "('team', 'NN')\n",
      "('since', 'IN')\n",
      "('2019', 'CD')\n",
      "(',', ',')\n",
      "('branding', 'VBG')\n",
      "('featured', 'VBN')\n",
      "('front', 'JJ')\n",
      "('team', 'NN')\n",
      "('’', 'NNP')\n",
      "('jersey', 'NN')\n",
      "('.', '.')\n",
      "('June', 'NNP')\n",
      "('last', 'JJ')\n",
      "('year', 'NN')\n",
      "(',', ',')\n",
      "(PERSON Byju/NNP)\n",
      "('’', 'NNP')\n",
      "('extended', 'VBD')\n",
      "('sponsorship', 'NN')\n",
      "('rights', 'NNS')\n",
      "(ORGANIZATION BCCI/NNP)\n",
      "('till', 'NN')\n",
      "('November', 'NNP')\n",
      "('.', '.')\n",
      "('ed-tech', 'JJ')\n",
      "('firm', 'NN')\n",
      "('asked', 'VBD')\n",
      "('cricket', 'NNS')\n",
      "('board', 'NN')\n",
      "('encash', 'VBP')\n",
      "('₹140', 'NNP')\n",
      "('crore', 'VBD')\n",
      "('bank', 'NN')\n",
      "('guarantee', 'NN')\n",
      "(',', ',')\n",
      "('remaining', 'VBG')\n",
      "('₹160', 'JJ')\n",
      "('crore', 'NN')\n",
      "('paid', 'VBD')\n",
      "('instalments', 'NNS')\n",
      "('.', '.')\n",
      "('November', 'NNP')\n",
      "(',', ',')\n",
      "(PERSON Byju/NNP)\n",
      "('’', 'NNP')\n",
      "('hinted', 'VBD')\n",
      "('possible', 'JJ')\n",
      "('talks', 'NNS')\n",
      "('settlement', 'NN')\n",
      "('two', 'CD')\n",
      "('.', '.')\n",
      "('However', 'RB')\n",
      "(',', ',')\n",
      "('company', 'NN')\n",
      "('facing', 'VBG')\n",
      "('severe', 'JJ')\n",
      "('financial', 'JJ')\n",
      "('crisis', 'NN')\n",
      "('unable', 'JJ')\n",
      "('repay', 'NN')\n",
      "('debts', 'NNS')\n",
      "('owed', 'VBN')\n",
      "('creditors', 'NNS')\n",
      "('.', '.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tr4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_filtered = [w for w in tokens if not w.lower() in stop_words]\n",
    "length = len(tokens_filtered)\n",
    "for i in range (1, length):\n",
    "\tif i in (',', '.', '_'):\n",
    "\t\ttokens_filtered.remove(tokens_filtered[i])\n",
    "    \n",
    "\n",
    "# Apply part-of-speech tagging to the tokens\n",
    "tagged = nltk.pos_tag(tokens_filtered)\n",
    "\n",
    "# Apply named entity recognition to the tagged words\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "# Print the entities found in the text\n",
    "for entity in entities:\n",
    "\tprint(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
